{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "281275d20b3c5f659236d880031e3da4b7977e06"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "ebb7ec8cdfe9b840f5b5ba1524f4e892994e46cc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from dlmslib.torch_models import trees, nlp_models\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "713132d2f045501c82e42290550d356ae0727c35"
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = '../input/'\n",
    "ORIGINAL_DATA_FOLDER = os.path.join(DATA_ROOT, 'movie-review-sentiment-analysis-kernels-only')\n",
    "TMP_DATA_FOLDER = os.path.join(DATA_ROOT, 'kaggle_review_sentiment_tmp_data')\n",
    "TREEBANK_DATA_FOLDER = os.path.join(DATA_ROOT, 'stanford-sentiment-treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "98fd884ce5265326d434498bda1f5bb813f24e95"
   },
   "outputs": [],
   "source": [
    "train_data_path = os.path.join(ORIGINAL_DATA_FOLDER, 'train.tsv')\n",
    "test_data_path = os.path.join(ORIGINAL_DATA_FOLDER, 'test.tsv')\n",
    "sub_data_path = os.path.join(ORIGINAL_DATA_FOLDER, 'sampleSubmission.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_data_path, sep=\"\\t\")\n",
    "test_df = pd.read_csv(test_data_path, sep=\"\\t\")\n",
    "sub_df = pd.read_csv(sub_data_path, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Tree Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledTextBinaryTreeNode(object):  # a node in the tree\n",
    "    def __init__(self, label, text=None):\n",
    "        self.label = label\n",
    "        self.text = text\n",
    "        self.left = None  # reference to left child\n",
    "        self.right = None  # reference to right child\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.is_leaf():\n",
    "            return '[{0}:{1}]'.format(self.text, self.label)\n",
    "        return '({0} <- [{1}:{2}] -> {3})'.format(self.left, self.text, self.label, self.right)\n",
    "\n",
    "    def is_leaf(self):\n",
    "        # true if we have finished performing fowardprop on this node (note,\n",
    "        # there are many ways to implement the recursion.. some might not\n",
    "        # require this flag)\n",
    "        return self.left is None and self.right is None\n",
    "\n",
    "    def get_leaf_texts(self):\n",
    "        # from left to right\n",
    "        if self.is_leaf():\n",
    "            return [self.text]\n",
    "        else:\n",
    "            return self.left.get_leaf_texts() + self.right.get_leaf_texts()\n",
    "\n",
    "    def get_transitions(self, shift_symbol='SHIFT', reduce_symbol='REDUCE'):\n",
    "        # from left to right\n",
    "        if self.is_leaf():\n",
    "            return [shift_symbol, ]\n",
    "        else:\n",
    "            return self.left.get_transitions(shift_symbol=shift_symbol, reduce_symbol=reduce_symbol) + \\\n",
    "                   self.right.get_transitions(shift_symbol=shift_symbol, reduce_symbol=reduce_symbol) + [reduce_symbol, ]\n",
    "\n",
    "    def get_non_leaf_labels_post_order(self):\n",
    "        if self.is_leaf():\n",
    "            return list()\n",
    "        else:\n",
    "            return self.left.get_non_leaf_labels_post_order() + \\\n",
    "                   self.right.get_non_leaf_labels_post_order() + [self.label]\n",
    "\n",
    "    def get_labels_in_transition_order(self):\n",
    "        if self.is_leaf():\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.left.get_labels_in_transition_order() + \\\n",
    "                   self.right.get_labels_in_transition_order() + [self.label]\n",
    "\n",
    "    def get_nodes_in_transition_order(self):\n",
    "        if self.is_leaf():\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.left.get_nodes_in_transition_order() + \\\n",
    "                   self.right.get_nodes_in_transition_order() + [self]\n",
    "\n",
    "    def get_leaf_labels(self):\n",
    "        # from left to right\n",
    "        if self.is_leaf():\n",
    "            return [self.label]\n",
    "        else:\n",
    "            return self.left.get_leaf_labels() + \\\n",
    "                   self.right.get_leaf_labels()\n",
    "\n",
    "    def get_leaf_nodes(self):\n",
    "        # from left to right\n",
    "        if self.is_leaf():\n",
    "            return [self]\n",
    "        else:\n",
    "            return self.left.get_leaf_nodes() + \\\n",
    "                   self.right.get_leaf_nodes()\n",
    "\n",
    "    @classmethod\n",
    "    def parse_ptb_string(cls, ptb_string, open_char='(', close_char=')'):\n",
    "        tokens = []\n",
    "        for toks in ptb_string.strip().split():\n",
    "            tokens += list(toks)\n",
    "        return LabeledTextBinaryTreeNode.__parse_ptb_tokens(tokens, open_char=open_char, close_char=close_char)\n",
    "\n",
    "    @classmethod\n",
    "    def __parse_ptb_tokens(cls, tokens, open_char='(', close_char=')'):\n",
    "        assert tokens[0] == open_char, \"Malformed tree\"\n",
    "        assert tokens[-1] == close_char, \"Malformed tree\"\n",
    "\n",
    "        split = 2  # position after open and label\n",
    "        count_open = count_close = 0\n",
    "\n",
    "        if tokens[split] == open_char:\n",
    "            count_open += 1\n",
    "            split += 1\n",
    "        # Find where left child and right child split\n",
    "        while count_open != count_close:\n",
    "            if tokens[split] == open_char:\n",
    "                count_open += 1\n",
    "            if tokens[split] == close_char:\n",
    "                count_close += 1\n",
    "            split += 1\n",
    "\n",
    "        # New node\n",
    "        node = LabeledTextBinaryTreeNode(int(tokens[1]))  # zero index labels\n",
    "\n",
    "        # leaf Node\n",
    "        if count_open == 0:\n",
    "            node.text = ''.join(tokens[2: -1]).lower()  # lower case?\n",
    "            node.isLeaf = True\n",
    "            return node\n",
    "\n",
    "        node.left = LabeledTextBinaryTreeNode.__parse_ptb_tokens(tokens[2: split], open_char=open_char, close_char=close_char)\n",
    "        node.right = LabeledTextBinaryTreeNode.__parse_ptb_tokens(tokens[split: -1], open_char=open_char, close_char=close_char)\n",
    "\n",
    "        return node\n",
    "\n",
    "\n",
    "def read_parse_ptb_tree_bank_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as fid:\n",
    "        tree_list = [LabeledTextBinaryTreeNode.parse_ptb_string(l) for l in fid.readlines()]\n",
    "    return tree_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TreeBank Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tree_bank_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as fid:\n",
    "        tree_list = [LabeledTextBinaryTreeNode.parse_ptb_string(l) for l in fid.readlines()]\n",
    "    return tree_list\n",
    "\n",
    "train_data_path = os.path.join(TREEBANK_DATA_FOLDER, 'train.txt')\n",
    "test_data_path = os.path.join(TREEBANK_DATA_FOLDER, 'test.txt')\n",
    "dev_data_path = os.path.join(TREEBANK_DATA_FOLDER, 'dev.txt')\n",
    "\n",
    "train_trees = read_tree_bank_file(train_data_path)\n",
    "test_trees = read_tree_bank_file(test_data_path)\n",
    "dev_trees = read_tree_bank_file(dev_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee07dfaceeb2d800b4c95430a5f44d7386fed119"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "abc33cf6801467c6c8ccd5266961d5e74a70b030"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import text as sktext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "33485dab7ac4678cc77fed13b2c8e500d341f72f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "c2e0e484aacbb21ebb2807c79beca5aa054c3830"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "641a74568482c8ad42d1ac1c8982a44b84641166"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  Sentiment\n",
       "0    156061          2\n",
       "1    156062          2\n",
       "2    156063          2\n",
       "3    156064          2\n",
       "4    156065          2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c4d3c60c638b8ed36a90102e2f5d0fcb034b9ac4"
   },
   "source": [
    "## Find Overlapped Phrases Between Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "0a31536ca50b348303c4980a7535fdea69716d6e"
   },
   "outputs": [],
   "source": [
    "overlapped = pd.merge(train_df[[\"Phrase\", \"Sentiment\"]], test_df, on=\"Phrase\", how=\"inner\")\n",
    "overlap_boolean_mask_test = test_df['Phrase'].isin(overlapped['Phrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22c203439a2310d54f0283dc4e117307a1afa82d"
   },
   "source": [
    "## Explore Sentence Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "c957e257b2865c258a6d9ce23b9ed15c316b90b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training and testing data sentences hist:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x129e9d278>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"training and testing data sentences hist:\")\n",
    "sns.distplot(train_df['SentenceId'], kde_kws={\"label\": \"train\"})\n",
    "sns.distplot(test_df['SentenceId'], kde_kws={\"label\": \"test\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6ae6308dd0bb586f7cc6e75619e228f7984116ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of overlapped SentenceId between training and testing data:\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The number of overlapped SentenceId between training and testing data:\")\n",
    "train_overlapped_sentence_id_df = train_df[train_df['SentenceId'].isin(test_df['SentenceId'])]\n",
    "print(train_overlapped_sentence_id_df.shape[0])\n",
    "\n",
    "del train_overlapped_sentence_id_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "030ab4a34391188a30160731259e21d4611762a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence and phrases: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26489</th>\n",
       "      <td>26490</td>\n",
       "      <td>1215</td>\n",
       "      <td>Oh , look at that clever angle !</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26490</th>\n",
       "      <td>26491</td>\n",
       "      <td>1215</td>\n",
       "      <td>Oh</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26491</th>\n",
       "      <td>26492</td>\n",
       "      <td>1215</td>\n",
       "      <td>, look at that clever angle !</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26492</th>\n",
       "      <td>26493</td>\n",
       "      <td>1215</td>\n",
       "      <td>look at that clever angle !</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26493</th>\n",
       "      <td>26494</td>\n",
       "      <td>1215</td>\n",
       "      <td>look at that clever angle</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26494</th>\n",
       "      <td>26495</td>\n",
       "      <td>1215</td>\n",
       "      <td>at that clever angle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26495</th>\n",
       "      <td>26496</td>\n",
       "      <td>1215</td>\n",
       "      <td>that clever angle</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26496</th>\n",
       "      <td>26497</td>\n",
       "      <td>1215</td>\n",
       "      <td>clever angle</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26497</th>\n",
       "      <td>26498</td>\n",
       "      <td>1215</td>\n",
       "      <td>angle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PhraseId  SentenceId                            Phrase  Sentiment\n",
       "26489     26490        1215  Oh , look at that clever angle !          3\n",
       "26490     26491        1215                                Oh          2\n",
       "26491     26492        1215     , look at that clever angle !          2\n",
       "26492     26493        1215       look at that clever angle !          4\n",
       "26493     26494        1215         look at that clever angle          3\n",
       "26494     26495        1215              at that clever angle          2\n",
       "26495     26496        1215                 that clever angle          3\n",
       "26496     26497        1215                      clever angle          3\n",
       "26497     26498        1215                             angle          2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 250\n",
    "print(\"Example of sentence and phrases: \")\n",
    "\n",
    "sample_sentence_id = train_df.sample(1)['SentenceId'].values[0]\n",
    "sample_sentence_group_df = train_df[train_df['SentenceId'] == sample_sentence_id]\n",
    "sample_sentence_group_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no overlapped sentence between training and testing data. Within each sentence group, the phraseId order is the in-order tanversal over the parsing tree of the sentence text. (This might be a very important information as we can utilized the composition as powerful predictive information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "09c94783b2b9f99c5d62ea251391642329ec46a6"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "f15786c0c632835ae7a008114319badd92dce4c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import gensim\n",
    "from sklearn import preprocessing as skp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "b50c845cca35a18a4f2986d03b37eb826c17e109"
   },
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "embed_size = 300\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5cd1af62c09ef764aed364830a0d2485ec9bb9af"
   },
   "source": [
    "## Find Text in the Tree Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we group the data by sentence id, the first phrase in the group is the original sentence text. We can use it to find the parsing tree in the sentiment tree bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_bank_texts = list(map(\n",
    "    lambda tree: \" \".join(tree.get_leaf_texts()).strip().lower(),\n",
    "    train_trees + dev_trees + test_trees\n",
    "))\n",
    "\n",
    "treebank_text_to_tree = dict(\n",
    "    list(zip(tree_bank_texts, train_trees + dev_trees + test_trees))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((((([this:2] <- [None:2] -> [idea:2]) <- [None:1] -> ([has:2] <- [None:1] -> ([lost:2] <- [None:1] -> ([its:2] <- [None:3] -> [originality:3])))) <- [None:2] -> [...:2]) <- [None:1] -> [and:2]) <- [None:1] -> (([neither:2] <- [None:2] -> [star:2]) <- [None:1] -> ([appears:2] <- [None:0] -> ([very:2] <- [None:1] -> ([excited:3] <- [None:1] -> ([at:2] <- [None:1] -> ([rehashing:2] <- [None:1] -> ([what:2] <- [None:2] -> (([was:2] <- [None:2] -> [basically:2]) <- [None:2] -> ([a:2] <- [None:2] -> ([one-joke:1] <- [None:2] -> [picture:2]))))))))))) <- [None:0] -> [.:2])\n"
     ]
    }
   ],
   "source": [
    "text = test_df[test_df['SentenceId'] == test_df.sample(1)['SentenceId'].values[0]].iloc[0]['Phrase'].strip().lower()\n",
    "print(treebank_text_to_tree[text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "506b2f699332c271c1f5250286b038a3f01fb9c9"
   },
   "source": [
    "## Find Parsing Trees for the Training Data and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8529\n",
      "8442\n"
     ]
    }
   ],
   "source": [
    "train_sents = train_df.groupby('SentenceId').apply(lambda chunk: chunk.iloc[0]['Phrase'].strip().lower())\n",
    "train_data_trees = list(map(\n",
    "    lambda sent: treebank_text_to_tree[sent] if sent in treebank_text_to_tree else None,\n",
    "    train_sents\n",
    "))\n",
    "\n",
    "print(len(train_data_trees))\n",
    "train_data_trees = list(filter(\n",
    "    lambda tree: tree is not None,\n",
    "    train_data_trees\n",
    "))\n",
    "print(len(train_data_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>2006</td>\n",
       "      <td>76</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>2007</td>\n",
       "      <td>76</td>\n",
       "      <td>Contrived pastiche of caper</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>2008</td>\n",
       "      <td>76</td>\n",
       "      <td>Contrived pastiche</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2009</td>\n",
       "      <td>76</td>\n",
       "      <td>Contrived</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>2010</td>\n",
       "      <td>76</td>\n",
       "      <td>pastiche</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2011</td>\n",
       "      <td>76</td>\n",
       "      <td>of caper</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2012</td>\n",
       "      <td>76</td>\n",
       "      <td>caper</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PhraseId  SentenceId                       Phrase  Sentiment\n",
       "2005      2006          76                                       1\n",
       "2006      2007          76  Contrived pastiche of caper          2\n",
       "2007      2008          76           Contrived pastiche          2\n",
       "2008      2009          76                    Contrived          1\n",
       "2009      2010          76                     pastiche          2\n",
       "2010      2011          76                     of caper          2\n",
       "2011      2012          76                        caper          2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['SentenceId'] == 76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3310\n",
      "3293\n"
     ]
    }
   ],
   "source": [
    "test_sents = test_df.groupby('SentenceId').apply(lambda chunk: chunk.iloc[0]['Phrase'].strip().lower())\n",
    "test_sent_ids = test_df.groupby('SentenceId').apply(lambda chunk: chunk.iloc[0]['SentenceId'])\n",
    "\n",
    "test_sent_ids = list(map(\n",
    "    lambda idx: test_sent_ids.iloc[idx] if test_sents.iloc[idx] in treebank_text_to_tree else None,\n",
    "    range(len(test_sent_ids))\n",
    "))\n",
    "test_data_trees = list(map(\n",
    "    lambda sent: treebank_text_to_tree[sent] if sent in treebank_text_to_tree else None,\n",
    "    test_sents\n",
    "))\n",
    "\n",
    "\n",
    "print(len(test_data_trees))\n",
    "test_data_trees = list(filter(\n",
    "    lambda tree: tree is not None,\n",
    "    test_data_trees\n",
    "))\n",
    "print(len(test_data_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr[:len(arr)-1], dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>15)\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_w2v_path = os.path.join(DATA_ROOT, \"fasttext-crawl-300d-2m/crawl-300d-2M.vec\")\n",
    "w2v_fasttext = load_embed(pretrained_w2v_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_TOKEN = '<UNK>'\n",
    "EMB_DIM = 300\n",
    "\n",
    "def map_unknown_token(tree, embeddings_index):\n",
    "    if tree is None:\n",
    "        return\n",
    "    \n",
    "    word = tree.text\n",
    "    if word not in embeddings_index:\n",
    "            tree.text = UNKNOWN_TOKEN\n",
    "    \n",
    "    map_unknown_token(tree.left, embeddings_index)\n",
    "    map_unknown_token(tree.right, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in train_trees:\n",
    "    map_unknown_token(tree, w2v_fasttext)\n",
    "for tree in test_trees:\n",
    "    map_unknown_token(tree, w2v_fasttext)\n",
    "for tree in dev_trees:\n",
    "    map_unknown_token(tree, w2v_fasttext)\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = list(set(flatten([t.get_leaf_texts() for t in (train_trees + test_trees + dev_trees)])))\n",
    "\n",
    "word2index = {'<UNK>': 0}\n",
    "wv = np.zeros(shape= (len(vocab), EMB_DIM))\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "        wv[word2index[vo]] = w2v_fasttext[vo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Seq-Tree LSTM Model With Thin Stack Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics as sm\n",
    "import torch\n",
    "import torch.nn.functional as tfunc\n",
    "from torch import autograd\n",
    "from torch import cuda as tcuda\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ThinStackHybridLSTM(nn.Module):\n",
    "    SHIFT_SYMBOL = 1\n",
    "    REDUCE_SYMBOL = 2\n",
    "\n",
    "    def __init__(self, embed_matrix, hidden_size, tracker_size, output_size, pad_token_index, alph_droput=0.5,\n",
    "                 trainable_embed=True,\n",
    "                 use_gpu=False, train_phase=True):\n",
    "        super(ThinStackHybridLSTM, self).__init__()\n",
    "\n",
    "        self.trainable_embed = trainable_embed\n",
    "        self.use_gpu = use_gpu\n",
    "        self.train_phase = train_phase\n",
    "        self.alph_dropout = alph_droput\n",
    "        self.pad_index = pad_token_index\n",
    "\n",
    "        if isinstance(embed_matrix, np.ndarray):\n",
    "            voc_size, embed_size = embed_matrix.shape\n",
    "            self.embed = nn.Embedding(voc_size, embed_size)\n",
    "            self.embed.weight = nn.Parameter(torch.from_numpy(embed_matrix).float())\n",
    "            self.embed.weight.requires_grad = trainable_embed\n",
    "        elif isinstance(embed_matrix, (int, np.int8, np.int16, np.int32, np.int64, np.int128)):\n",
    "            embed_size = embed_matrix\n",
    "            voc_size = hidden_size\n",
    "            self.embed = nn.Embedding(voc_size, embed_size)\n",
    "            self.embed.weight.requires_grad = trainable_embed\n",
    "        else:\n",
    "            raise ValueError(\"embed matrix must be either 2d numpy array or integer\")\n",
    "\n",
    "        self.W_in = nn.Linear(embed_size, hidden_size)\n",
    "        self.reduce = Reduce(hidden_size, tracker_size)\n",
    "        self.tracker = Tracker(hidden_size, tracker_size)\n",
    "        self.W_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, token_index_sequences, transitions):\n",
    "\n",
    "        buffers = self.embed(token_index_sequences)\n",
    "        buffers = tfunc.alpha_dropout(tfunc.selu(self.W_in(buffers)), self.alph_dropout, training=self.train_phase)\n",
    "\n",
    "        outputs0 = tfunc.log_softmax(self.W_out(buffers), 2).transpose(1, 0)\n",
    "\n",
    "        buffers = [\n",
    "            list(torch.split(b.squeeze(0), 1, 0))[::-1]\n",
    "            for b in torch.split(buffers, 1, 0)\n",
    "        ]\n",
    "\n",
    "        transitions.transpose_(1, 0)\n",
    "\n",
    "        # The input comes in as a single tensor of word embeddings;\n",
    "        # I need it to be a list of stacks, one for each example in\n",
    "        # the batch, that we can pop from independently. The words in\n",
    "        # each example have already been reversed, so that they can\n",
    "        # be read from left to right by popping from the end of each\n",
    "        # list; they have also been prefixed with a null value.\n",
    "\n",
    "        # shape = (max_len, batch, embed_dims)\n",
    "        buffers = [list(map(lambda vec_: torch.cat([vec_, vec_], 1), buf)) for buf in buffers]\n",
    "\n",
    "        pad_embed = buffers[0][0]\n",
    "        stacks = [[pad_embed, pad_embed] for _ in buffers]\n",
    "\n",
    "        self.tracker.reset_state()\n",
    "\n",
    "        # TODO\n",
    "        # shape = (max_len, batch)\n",
    "        num_transitions = transitions.size(0)\n",
    "\n",
    "        outputs1 = list()\n",
    "        for i in range(num_transitions):\n",
    "            trans = transitions[i]\n",
    "            tracker_states = self.tracker(buffers, stacks)\n",
    "\n",
    "            lefts, rights, trackings = [], [], []\n",
    "            batch = list(zip(trans.data, buffers, stacks, tracker_states))\n",
    "\n",
    "            for bi in range(len(batch)):\n",
    "                transition, buf, stack, tracking = batch[bi]\n",
    "                if transition == self.SHIFT_SYMBOL:  # shift\n",
    "                    stack.append(buf.pop())\n",
    "                elif transition == self.REDUCE_SYMBOL:  # reduce\n",
    "                    rights.append(stack.pop())\n",
    "                    lefts.append(stack.pop())\n",
    "                    trackings.append(tracking)\n",
    "\n",
    "                # make sure tree are good\n",
    "                while len(stack) < 2:\n",
    "                    stack.append(pad_embed)\n",
    "\n",
    "            if rights:\n",
    "                hc_list, hc_tensor = self.reduce(lefts, rights, trackings)\n",
    "                reduced = iter(hc_list)\n",
    "                for transition, stack in zip(trans.data, stacks):\n",
    "                    if transition == 2:\n",
    "                        stack.append(next(reduced))\n",
    "\n",
    "                outputs1.append(tfunc.log_softmax(self.W_out(hc_tensor[0]), 1))\n",
    "\n",
    "        # shape2 = (max_len, batch_size, output_dim)\n",
    "        # shape1 = (max_len, [num_reduce], output_dim)\n",
    "        return outputs0, outputs1\n",
    "\n",
    "    def predict_label_for_trees(self, trees, train_phase=False):\n",
    "        if not hasattr(self, 'word2index_'):\n",
    "            raise AttributeError('train the model from trees first!')\n",
    "\n",
    "        max_len = 0\n",
    "        for tree in trees:\n",
    "            max_len = max(len(tree.get_leaf_texts()), max_len)\n",
    "\n",
    "        data = ThinStackHybridLSTM.prepare_data(\n",
    "            trees, self.word2index_, max_len=max_len, pre_pad_index=self.pad_index, post_pad_index=self.pad_index, for_train=False\n",
    "        )\n",
    "\n",
    "        self.train_phase = train_phase\n",
    "        preds, nodes = self._predict_and_pack_tensor(\n",
    "            data[0], data[1], data[2], data[3], for_train=False\n",
    "        )\n",
    "\n",
    "        preds = preds.max(1)[1].data.tolist()\n",
    "        for i in range(len(preds)):\n",
    "            nodes[i].label = preds[i]\n",
    "\n",
    "    def train_model_from_trees(self, train_trees, word2index, max_len, validation_trees=None, epochs=30, batch_size=32):\n",
    "        self.word2index_ = word2index\n",
    "\n",
    "        train_data = ThinStackHybridLSTM.prepare_data(\n",
    "            train_trees, word2index, max_len=max_len, pre_pad_index=self.pad_index, post_pad_index=self.pad_index\n",
    "        )\n",
    "\n",
    "        if validation_trees is not None:\n",
    "            valid_data = ThinStackHybridLSTM.prepare_data(\n",
    "                validation_trees, word2index, max_len=max_len, pre_pad_index=self.pad_index,\n",
    "                post_pad_index=self.pad_index\n",
    "            )\n",
    "        else:\n",
    "            valid_data = [None] * 4\n",
    "\n",
    "        self.train_model(train_data[0], train_data[1], train_data[2], train_data[3],\n",
    "                         epochs=epochs, batch_size=batch_size,\n",
    "                         validation_tokens=valid_data[0], validation_transitions=valid_data[1],\n",
    "                         validation_labels=valid_data[2], validation_token_labels=valid_data[3])\n",
    "\n",
    "    def train_model(self, train_tokens, train_transitions, train_labels, train_token_labels,\n",
    "                    epochs=100, batch_size=30,\n",
    "                    validation_tokens=None, validation_transitions=None, validation_labels=None,\n",
    "                    validation_token_labels=None):\n",
    "\n",
    "        def get_batch(train_tokens_, train_transitions_, train_labels_, train_token_labels_, batch_size_=batch_size):\n",
    "            indices = np.arange(0, train_tokens.shape[0], step=1, dtype=np.int32).tolist()\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            train_tokens_ = train_tokens_[indices]\n",
    "            train_transitions_ = train_transitions_[indices]\n",
    "            train_labels_ = list(map(\n",
    "                lambda i_: train_labels_[i_],\n",
    "                indices\n",
    "            ))\n",
    "            train_token_labels_ = list(map(\n",
    "                lambda i_: train_token_labels_[i_],\n",
    "                indices\n",
    "            ))\n",
    "\n",
    "            idx = 0\n",
    "            while idx < train_tokens_.shape[0]:\n",
    "                end_idx = min(idx + batch_size_, train_tokens_.shape[0])\n",
    "                batch_tokens_ = train_tokens_[idx: end_idx]\n",
    "                batch_trans_ = train_transitions_[idx: end_idx]\n",
    "                batch_labels_ = train_labels_[idx: end_idx]\n",
    "                batch_token_labels_ = train_token_labels_[idx: end_idx]\n",
    "                idx = end_idx\n",
    "                yield batch_tokens_, batch_trans_, batch_labels_, batch_token_labels_\n",
    "\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.RMSprop(self.parameters())\n",
    "        self.train_phase = True\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "\n",
    "            batch_index = 0\n",
    "            for batch_tokens, batch_transitions, batch_labels, batch_token_labels in \\\n",
    "                    get_batch(train_tokens, train_transitions, train_labels, train_token_labels):\n",
    "                preds, labels = self._predict_and_pack_tensor(\n",
    "                    batch_tokens, batch_transitions, batch_labels, batch_token_labels\n",
    "                )\n",
    "\n",
    "                loss = loss_func(preds, labels)\n",
    "                losses.append(loss.data.tolist())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if batch_index % 100 == 0:\n",
    "                    preds, labels = preds.max(1)[1].data.tolist(), labels.data.tolist()\n",
    "                    prec_score = sm.precision_score(labels, preds, average='weighted')\n",
    "                    reca_score = sm.recall_score(labels, preds, average='weighted')\n",
    "\n",
    "                    print('[%d/%d] mean_loss: %.4f; weighted_precision: %.4f; weighted_recall: %.4f' % (\n",
    "                    epoch, epochs, np.mean(losses), prec_score, reca_score))\n",
    "                    losses = []\n",
    "\n",
    "                batch_index += 1\n",
    "\n",
    "            if validation_labels is not None and \\\n",
    "                    validation_token_labels is not None and \\\n",
    "                    validation_tokens is not None and \\\n",
    "                    validation_transitions is not None:\n",
    "                preds, labels = self._predict_and_pack_tensor(\n",
    "                    validation_tokens, validation_transitions, validation_labels, validation_token_labels\n",
    "                )\n",
    "\n",
    "                preds, labels = preds.max(1)[1].data.tolist(), labels.data.tolist()\n",
    "                print(sm.classification_report(labels, preds))\n",
    "\n",
    "    def _predict_and_pack_tensor(self, batch_tokens, batch_transitions, batch_labels, batch_token_labels, for_train=True):\n",
    "\n",
    "        batch_tokens = torch.from_numpy(batch_tokens)\n",
    "        batch_transitions = torch.from_numpy(batch_transitions)\n",
    "\n",
    "        LongTensor = tcuda.LongTensor if self.use_gpu else torch.LongTensor\n",
    "        model = self.cuda() if self.use_gpu else self\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "        model.zero_grad()\n",
    "        batch_token_pred, batch_pred = model(batch_tokens, batch_transitions)\n",
    "\n",
    "        preds_list = list()\n",
    "        label_list = list()\n",
    "        # -------- add token prediction ---------\n",
    "        # [batch_size, max_len, [1, vec_dim]]\n",
    "        batch_token_pred_list = list(map(\n",
    "            lambda token_pred: list(map(\n",
    "                lambda vec: vec,\n",
    "                torch.split(token_pred.squeeze(1), 1, 0)\n",
    "            )),\n",
    "            torch.split(batch_token_pred, 1, 1)\n",
    "        ))\n",
    "        # flatten\n",
    "        # TODO: test\n",
    "        # print((len(batch_token_pred_list), len(batch_token_pred_list[0])))\n",
    "        # print((len(batch_token_labels), len(batch_token_labels[0])))\n",
    "\n",
    "        batch_token_pred_list = flatten(batch_token_pred_list)\n",
    "        batch_token_labels = flatten(batch_token_labels)\n",
    "\n",
    "        # filter out padding leaf nodes\n",
    "        for ti in range(len(batch_token_pred_list)):\n",
    "            if batch_token_labels[ti] is not None:\n",
    "                preds_list.append(batch_token_pred_list[ti])\n",
    "                label_list.append(batch_token_labels[ti])\n",
    "\n",
    "        # -------- add prediction ---------\n",
    "        for li in range(len(batch_labels[0])):\n",
    "            for bi in range(len(batch_labels)):\n",
    "                if batch_labels[bi][li] is not None:\n",
    "                    label_list.append(batch_labels[bi][li])\n",
    "\n",
    "        # [max_len, [num_reduces], [1, vec_dim]]\n",
    "        batch_pred = list(map(\n",
    "            lambda preds: torch.split(preds, 1, 0),\n",
    "            batch_pred\n",
    "        ))\n",
    "        batch_pred = flatten(batch_pred)\n",
    "        preds_list.extend(batch_pred)\n",
    "\n",
    "\n",
    "        # make them tensors\n",
    "        preds = torch.cat(preds_list, 0)\n",
    "        if for_train:\n",
    "            labels = autograd.Variable(LongTensor(np.asarray(label_list, dtype=np.int32)))\n",
    "        else:\n",
    "            labels = label_list\n",
    "\n",
    "        # [token_labels, non_leaf_labels]\n",
    "        return preds, labels\n",
    "\n",
    "    @classmethod\n",
    "    def prepare_data(cls, trees, word2index, max_len, pre_pad_index, post_pad_index, for_train=True):\n",
    "        max_len_tran = 2 * max_len - 1\n",
    "\n",
    "        words_batch, transitions_batch = list(), list()\n",
    "        non_leaf_labels_batch, leaf_labels_batch = list(), list()\n",
    "\n",
    "        for tree in trees:\n",
    "            words, transitions, non_leaf_labels, leaf_labels = cls.__from_tree(\n",
    "                tree, word2index, max_len_tran, pre_pad=pre_pad_index, post_pad=post_pad_index, for_train=for_train)\n",
    "            words_batch.append(words)\n",
    "            transitions_batch.append(transitions)\n",
    "\n",
    "            non_leaf_labels_batch.append(non_leaf_labels)\n",
    "            leaf_labels_batch.append(leaf_labels)\n",
    "\n",
    "        words_batch = np.array(words_batch)\n",
    "        transitions_batch = np.array(transitions_batch)\n",
    "\n",
    "        return words_batch, transitions_batch, non_leaf_labels_batch, leaf_labels_batch\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __from_tree(tree, word2index, max_len_tran, pre_pad, post_pad, for_train=True):\n",
    "        words = tree.get_leaf_texts()\n",
    "\n",
    "        if word2index is not None:\n",
    "            words = list(map(lambda word: word2index[word], words))\n",
    "\n",
    "        transitions = tree.get_transitions(\n",
    "            shift_symbol=ThinStackHybridLSTM.SHIFT_SYMBOL, reduce_symbol=ThinStackHybridLSTM.REDUCE_SYMBOL)\n",
    "\n",
    "        if for_train:\n",
    "            non_leaf_labels = tree.get_labels_in_transition_order()\n",
    "            leaf_labels = tree.get_leaf_labels()\n",
    "        else:\n",
    "            non_leaf_labels = tree.get_nodes_in_transition_order()\n",
    "            leaf_labels = tree.get_leaf_nodes()\n",
    "\n",
    "        num_words = len(words)\n",
    "        num_transitions = len(transitions)\n",
    "\n",
    "        if len(transitions) <= max_len_tran:\n",
    "            # pad transitions with shift\n",
    "            num_pad_shifts = max_len_tran - num_transitions\n",
    "            transitions = [ThinStackHybridLSTM.SHIFT_SYMBOL, ] * num_pad_shifts + transitions\n",
    "            words = [pre_pad] * num_pad_shifts + \\\n",
    "                    words + \\\n",
    "                    [post_pad] * (max_len_tran - num_pad_shifts - num_words)\n",
    "\n",
    "            # leaf_labels should has the same length as words\n",
    "            # pad with None\n",
    "            leaf_labels = [None] * num_pad_shifts + \\\n",
    "                          leaf_labels + \\\n",
    "                          [None] * (max_len_tran - num_pad_shifts - num_words)\n",
    "\n",
    "            # non_leaf_labels must follow the same operations as transitions\n",
    "            non_leaf_labels = [None, ] * num_pad_shifts + non_leaf_labels\n",
    "\n",
    "\n",
    "        elif len(transitions) > max_len_tran:\n",
    "            num_shift_before_crop = num_words\n",
    "\n",
    "            transitions = transitions[len(transitions) - max_len_tran:]\n",
    "            # non_leaf_labels must follow the same operations as transitions\n",
    "            non_leaf_labels = non_leaf_labels[len(non_leaf_labels) - max_len_tran:]\n",
    "\n",
    "\n",
    "            trans = np.asarray(transitions)\n",
    "            num_shift_after_crop = np.sum(trans[trans == ThinStackHybridLSTM.SHIFT_SYMBOL])\n",
    "\n",
    "            words = words[num_shift_before_crop - num_shift_after_crop:]\n",
    "            words = words + [post_pad, ] * (max_len_tran - len(words))\n",
    "\n",
    "            # leaf_labels should has the same length as words\n",
    "            # pad with None\n",
    "            leaf_labels = leaf_labels[num_shift_before_crop - num_shift_after_crop:]\n",
    "            leaf_labels = leaf_labels + [None] * (max_len_tran - len(leaf_labels))\n",
    "\n",
    "        # pre-pad every data with one empty tokens and shift\n",
    "        transitions = [ThinStackHybridLSTM.SHIFT_SYMBOL, ] + transitions\n",
    "        words = [pre_pad, ] + words\n",
    "\n",
    "        leaf_labels = [None] + leaf_labels\n",
    "        non_leaf_labels = [None] + non_leaf_labels\n",
    "\n",
    "        return words, transitions, non_leaf_labels, leaf_labels\n",
    "\n",
    "    def init_weight(self):\n",
    "        if self.trainable_embed:\n",
    "            nn.init.xavier_uniform(self.embed.state_dict()['weight'])\n",
    "\n",
    "        nn.init.xavier_uniform(self.W_out.state_dict()['weight'])\n",
    "        nn.init.xavier_uniform(self.W_in.state_dict()['weight'])\n",
    "\n",
    "\n",
    "class Reduce(nn.Module):\n",
    "    \"\"\"TreeLSTM composition module for SPINN.\n",
    "    The TreeLSTM has two or three inputs: the first two are the left and right\n",
    "    children being composed; the third is the current state of the tracker\n",
    "    LSTM if one is present in the SPINN model.\n",
    "    Args:\n",
    "        size: The size of the model state.\n",
    "        tracker_size: The size of the tracker LSTM hidden state, or None if no\n",
    "            tracker is present.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, tracker_size):\n",
    "        super(Reduce, self).__init__()\n",
    "        self.left = nn.Linear(size, 5 * size)\n",
    "        self.right = nn.Linear(size, 5 * size, bias=False)\n",
    "        self.track = nn.Linear(tracker_size, 5 * size, bias=False)\n",
    "\n",
    "    def forward(self, left_in, right_in, tracking=None):\n",
    "        \"\"\"Perform batched TreeLSTM composition.\n",
    "        This implements the REDUCE operation of a SPINN in parallel for a\n",
    "        batch of nodes. The batch size is flexible; only provide this function\n",
    "        the nodes that actually need to be REDUCEd.\n",
    "        The TreeLSTM has two or three inputs: the first two are the left and\n",
    "        right children being composed; the third is the current state of the\n",
    "        tracker LSTM if one is present in the SPINN model. All are provided as\n",
    "        iterables and batched internally into tensors.\n",
    "        Additionally augments each new node with pointers to its children.\n",
    "        Args:\n",
    "            left_in: Iterable of ``B`` ~autograd.Variable objects containing\n",
    "                ``c`` and ``h`` concatenated for the left child of each node\n",
    "                in the batch.\n",
    "            right_in: Iterable of ``B`` ~autograd.Variable objects containing\n",
    "                ``c`` and ``h`` concatenated for the right child of each node\n",
    "                in the batch.\n",
    "            tracking: Iterable of ``B`` ~autograd.Variable objects containing\n",
    "                ``c`` and ``h`` concatenated for the tracker LSTM state of\n",
    "                each node in the batch, or None.\n",
    "        Returns:\n",
    "            out: Tuple of ``B`` ~autograd.Variable objects containing ``c`` and\n",
    "                ``h`` concatenated for the LSTM state of each new node. These\n",
    "                objects are also augmented with ``left`` and ``right``\n",
    "                attributes.\n",
    "        \"\"\"\n",
    "        left, right = _bundle(left_in), _bundle(right_in)\n",
    "        tracking = _bundle(tracking)\n",
    "        lstm_in = self.left(left[0])\n",
    "        lstm_in += self.right(right[0])\n",
    "        lstm_in += self.track(tracking[0])\n",
    "        hcs = Reduce.tree_lstm(left[1], right[1], lstm_in)\n",
    "        out = _unbundle(hcs)\n",
    "        return out, hcs\n",
    "\n",
    "    @classmethod\n",
    "    def tree_lstm(cls, c1, c2, lstm_in):\n",
    "        a, i, f1, f2, o = lstm_in.chunk(5, 1)\n",
    "        c = a.tanh() * i.sigmoid() + f1.sigmoid() * c1 + f2.sigmoid() * c2\n",
    "        h = o.sigmoid() * c.tanh()\n",
    "        return h, c\n",
    "\n",
    "\n",
    "class Tracker(nn.Module):\n",
    "\n",
    "    def __init__(self, size, tracker_size):\n",
    "        super(Tracker, self).__init__()\n",
    "        self.rnn = nn.LSTMCell(3 * size, tracker_size)\n",
    "        self.state_size = tracker_size\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.state = None\n",
    "\n",
    "    def forward(self, bufs, stacks):\n",
    "        buf = _bundle([buf[-1] for buf in bufs])[0]\n",
    "        stack1 = _bundle(stack[-1] for stack in stacks)[0]\n",
    "        stack2 = _bundle(stack[-2] for stack in stacks)[0]\n",
    "        x = torch.cat((buf, stack1, stack2), 1)\n",
    "        if self.state is None:\n",
    "            self.state = 2 * [autograd.Variable(\n",
    "                x.data.new(x.size(0), self.state_size).zero_())]\n",
    "        self.state = self.rnn(x, self.state)\n",
    "        return _unbundle(self.state)\n",
    "\n",
    "\n",
    "def _bundle(states):\n",
    "    if states is None:\n",
    "        return None\n",
    "    states = tuple(states)\n",
    "    if states[0] is None:\n",
    "        return None\n",
    "\n",
    "    # states is a list of B tensors of dimension (1, 2H)\n",
    "    # this returns 2 tensors of dimension (B, H)\n",
    "    return torch.cat(states, 0).chunk(2, 1)\n",
    "\n",
    "\n",
    "def _unbundle(state):\n",
    "    if state is None:\n",
    "        return itertools.repeat(None)\n",
    "    # state is a pair of tensors of dimension (B, H)\n",
    "    # this returns a list of B tensors of dimension (1, 2H)\n",
    "    return torch.split(torch.cat(state, 1), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 0\n",
    "for tree in (train_trees + test_trees + dev_trees):\n",
    "    MAX_LEN = max(len(tree.get_leaf_texts()), MAX_LEN)\n",
    "\n",
    "hidden_size = 100\n",
    "tracker_size = 100\n",
    "output_size = 5\n",
    "pad_token_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ThinStackHybridLSTM(wv, hidden_size, tracker_size, output_size, pad_token_index, trainable_embed=True, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1] mean_loss: 1.7061; weighted_precision: 0.6274; weighted_recall: 0.1561\n",
      "[0/1] mean_loss: 0.6835; weighted_precision: 0.7620; weighted_recall: 0.7690\n",
      "[0/1] mean_loss: 0.5383; weighted_precision: 0.7478; weighted_recall: 0.7612\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.21      0.31      3065\n",
      "          1       0.57      0.47      0.52     13809\n",
      "          2       0.85      0.94      0.90     84392\n",
      "          3       0.62      0.54      0.58     16675\n",
      "          4       0.69      0.33      0.45      5420\n",
      "\n",
      "avg / total       0.77      0.79      0.78    123361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train_model_from_trees(train_data_trees, word2index, MAX_LEN, validation_trees=test_data_trees, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Prediction for Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_label_for_trees(test_data_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 2, 2, 3, 2, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_trees[0].get_non_leaf_labels_post_order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_label(root, text_label_dict):\n",
    "    if root.is_leaf():\n",
    "        text_label_dict[str(root.text)] = root.label\n",
    "        return root.text\n",
    "\n",
    "    left_text = get_text_label(root.left, text_label_dict)\n",
    "    right_text = get_text_label(root.right, text_label_dict)\n",
    "    text = left_text + \" \" + right_text\n",
    "    \n",
    "    if text in text_label_dict and root.label != text_label_dict[text]:\n",
    "        print(text + \": \" + str(root.label) + \", \" + str(text_label_dict[text]))\n",
    "    \n",
    "    text_label_dict[str(text)] = root.label\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inconsistent Prediction: \n",
      "grandeur and: 2, 3\n",
      "few laughs: 2, 3\n",
      "no sense: 1, 2\n",
      "a great ending: 4, 3\n",
      "own right: 3, 2\n",
      "no sense: 2, 1\n",
      "few laughs: 3, 2\n",
      "to enjoy: 4, 3\n",
      "pretty good: 4, 3\n",
      "more satisfying: 4, 3\n",
      "moldy and: 2, 1\n",
      "lil bow wow: 2, 3\n",
      "too hard: 2, 1\n",
      "an original: 2, 3\n"
     ]
    }
   ],
   "source": [
    "print('Inconsistent Prediction: ')\n",
    "\n",
    "text_label_map = dict()\n",
    "for tree in test_data_trees:\n",
    "    get_text_label(tree, text_label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Result to Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = test_df['Phrase'].map(\n",
    "    lambda phrase_text: text_label_map[phrase_text.strip().lower()] if phrase_text.strip().lower() in text_label_map else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of phrases not found in the testing trees: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"the number of phrases not found in the testing trees: \")\n",
    "pd.isna(sentiments).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments.fillna(2, inplace=True)\n",
    "sub_df[\"Sentiment\"] = sentiments\n",
    "\n",
    "sub_not_overlap_df = sub_df[~overlap_boolean_mask_test]\n",
    "res_df = pd.concat([overlapped, sub_not_overlap_df], sort=True)[sub_df.columns.values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
